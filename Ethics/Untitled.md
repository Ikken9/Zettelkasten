# COMPAS

Correctional Offender Management Profiling for Alternative Sanctions, es una herramienta para el manejo de casos y la toma de decisiones desarrollado por Northpointe, utilizado por los Estados Unidos para evaluar la probabilidad de que la persona defendida se vuelva un reincidente.

Para evaluar el riesgo de reincidencia COMPAS utiliza un algoritmo que toma como referencia una serie de escalas de reincidencia general y violenta. Las escalas son hechas con datos provenientes de perfiles de delincuentes.

## Criticas
El algoritmo utilizado por COMPAS es secreto, lo que implica que un acusado no puede ver como es que su caso es evaluado, simplemente conoce el resultado.

Otra critica, es que el algoritmo es dependiente de datos para funcionar, si los datos con los que se entrena al algoritmo son erróneos o manipulados, los resultados de las evaluaciones pueden ser incorrectos.

## Precision
En 2016, una [investigación realizada por ProPublica](https://www.propublica.org/espanol), una organización periodística sin ánimo de lucro, reveló que **COMPAS tendía a sobreestimar el riesgo de reincidencia** entre las personas de raza negra, mientras que subestimaba el riesgo para las personas de raza blanca. El estudio demostró que las personas negras etiquetadas como de “alto riesgo” tenían casi el doble de probabilidades de no reincidir en comparación con las personas blancas con la misma clasificación. Del mismo modo, las personas blancas etiquetadas como de “bajo riesgo” tenían más probabilidades de reincidir que las personas negras clasificadas de la misma forma.

El problema del sesgo en COMPAS tiene sus raíces en los datos históricos que se usaron para entrenar el algoritmo. El sistema se alimentó de datos que reflejaban la discriminación racial y las desigualdades históricas en el sistema judicial estadounidense. Estos datos, inherentemente sesgados, **generaron un algoritmo que perpetúa las mismas desigualdades que pretendía combatir**.


El caso de COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) es un claro ejemplo de los dilemas éticos que surgen cuando se utiliza inteligencia artificial en la toma de decisiones que afectan vidas humanas. COMPAS es un algoritmo utilizado en el sistema judicial estadounidense para evaluar el riesgo de reincidencia de los delincuentes, y sus decisiones pueden influir en sentencias, libertad condicional y otras áreas críticas. Sin embargo, se ha demostrado que COMPAS puede ser sesgado, especialmente contra personas de raza negra, lo que plantea serias preocupaciones sobre la justicia y la equidad.

### Preguntas Éticas Clave:

1. **¿Podemos confiar en la IA para decisiones judiciales?** La IA puede procesar grandes cantidades de datos de manera eficiente, pero esto no garantiza que sus decisiones sean justas. Los algoritmos pueden reflejar sesgos inherentes en los datos de entrenamiento, perpetuando desigualdades existentes. La falta de transparencia en cómo estos algoritmos llegan a sus conclusiones también dificulta la confianza en ellos.
    
2. **¿Cómo asegurar que los sistemas de IA sean justos y equitativos?** La equidad en la IA requiere un diseño cuidadoso, desde la selección de datos hasta la implementación y supervisión continua. Es crucial que los desarrolladores y las autoridades supervisen activamente estos sistemas, identificando y corrigiendo posibles sesgos.
    
3. **¿Quién es responsable de los errores algorítmicos?** La responsabilidad en caso de errores es un tema complejo. ¿Es el programador que diseñó el algoritmo? ¿La empresa que lo comercializa? ¿O el sistema judicial que decide utilizarlo? Es fundamental que se establezcan marcos legales claros que definan la responsabilidad en estos casos.
    

### Reflexión Sobre el Futuro de la IA y la Justicia:

Como futuros ingenieros y profesionales en tecnología, tenemos una responsabilidad ética en el desarrollo y aplicación de la IA. Es vital que participemos en debates éticos sobre cómo estas tecnologías pueden impactar la justicia y la igualdad. La tecnología tiene el potencial de ser una herramienta poderosa para el progreso, pero también puede perpetuar la opresión si no se maneja con cuidado.

Para evitar que la IA se convierta en una herramienta de opresión, debemos:

- **Promover la transparencia**: Los sistemas de IA deben ser transparentes, permitiendo que sus decisiones puedan ser auditadas y entendidas por humanos.
    
- **Garantizar la diversidad en los datos de entrenamiento**: Los datos utilizados para entrenar algoritmos deben ser representativos y libres de sesgos que puedan distorsionar los resultados.
    
- **Establecer marcos éticos y regulatorios sólidos**: Es esencial que existan regulaciones que guíen el desarrollo y la implementación de IA, asegurando que se utilicen de manera justa y equitativa.
    
- **Fomentar la educación ética en tecnología**: Los ingenieros y desarrolladores deben estar formados en ética, entendiendo las implicaciones sociales y humanas de su trabajo.
    

En resumen, la IA tiene un potencial increíble, pero también plantea desafíos significativos. Es nuestra responsabilidad asegurarnos de que se utilice para promover la justicia y la igualdad, y no para perpetuar la discriminación y la injusticia.
